{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             note           category\n",
      "0       0075 0084 lunas nota baru 4pc r15h5 lgs06           Pinjaman\n",
      "1                                1 ayam tgl 9 des  Makanan & Minuman\n",
      "2            1 jt dp kerjaan 1 juta dp sewa mobil            Tagihan\n",
      "3   1 juta dp makanan minggu 250 rb uang mingguan  Makanan & Minuman\n",
      "4                                     1 kg daging  Makanan & Minuman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_l/ys4bqpxn2s956k6fljb7mksm0000gn/T/ipykernel_7452/2587950870.py:27: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['label'] = df['category'].replace(label_dict)\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indolem/indobertweet-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/badraaji/anaconda3/envs/Core-ML/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/1:   0%|          | 345/342015 [10:29<178:46:34,  1.88s/it, loss=1.86] "
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm  \n",
    "\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename='train_model.log', level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Setup local dataset path\n",
    "file_path = '../Bert/dataSet/notes1.csv'  # Ganti dengan path lokal Anda\n",
    "df = pd.read_csv(file_path)\n",
    "print(df.head())\n",
    "\n",
    "# Label encoding\n",
    "label_dict = {label: idx for idx, label in enumerate(df['category'].unique())}\n",
    "df['label'] = df['category'].replace(label_dict)\n",
    "\n",
    "# Split dataset into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df.index.values, df['label'].values, test_size=0.15, random_state=42, stratify=df['label'].values\n",
    ")\n",
    "\n",
    "df['data_type'] = 'not_set'\n",
    "df.loc[X_train, 'data_type'] = 'train'\n",
    "df.loc[X_val, 'data_type'] = 'val'\n",
    "\n",
    "# Tokenize data\n",
    "tokenizer = BertTokenizer.from_pretrained('indolem/indobert-base-uncased', do_lower_case=True)\n",
    "\n",
    "def encode_data(df, data_type):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        df[df['data_type'] == data_type]['note'].values,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        padding='max_length',  \n",
    "        truncation=True,  \n",
    "        max_length=256,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "\n",
    "encoded_data_train = encode_data(df, 'train')\n",
    "encoded_data_val = encode_data(df, 'val')\n",
    "\n",
    "input_ids_train, attention_masks_train = encoded_data_train['input_ids'], encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(df[df['data_type'] == 'train']['label'].values)\n",
    "\n",
    "input_ids_val, attention_masks_val = encoded_data_val['input_ids'], encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(df[df['data_type'] == 'val']['label'].values)\n",
    "\n",
    "# Create TensorDatasets\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "\n",
    "# Setup DataLoader\n",
    "batch_size = 3\n",
    "dataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=batch_size)\n",
    "dataloader_val = DataLoader(dataset_val, sampler=SequentialSampler(dataset_val), batch_size=batch_size)\n",
    "\n",
    "# Load BERT model for sequence classification with pytorch\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"indolem/indobertweet-base-uncased\", num_labels=len(label_dict), output_attentions=False, output_hidden_states=False\n",
    ")\n",
    "\n",
    "# Setup optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)\n",
    "epochs = 1\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=0, num_training_steps=len(dataloader_train) * epochs\n",
    ")\n",
    "\n",
    "# Define evaluation metrics\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions, true_vals = [], []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**{'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]})\n",
    "        loss, logits = outputs[:2]\n",
    "        total_loss += loss.item()\n",
    "        predictions.append(logits.detach().cpu().numpy())\n",
    "        true_vals.append(batch[2].cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "    return avg_loss, predictions, true_vals\n",
    "\n",
    "# Train and evaluate model\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    # Initialize tqdm progress bar\n",
    "    progress_bar = tqdm(dataloader_train, desc=f'Epoch {epoch}/{epochs}', leave=False, disable=False)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        model.zero_grad()\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        outputs = model(**{'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]})\n",
    "        loss = outputs[0]\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Update tqdm progress bar description with the current loss\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(dataloader_train)\n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_val)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "\n",
    "    # Log metrics\n",
    "    logger.info(f'Epoch {epoch}/{epochs}')\n",
    "    logger.info(f'Training Loss: {avg_train_loss}')\n",
    "    logger.info(f'Validation Loss: {val_loss}')\n",
    "    logger.info(f'Validation F1 Score: {val_f1}')\n",
    "\n",
    "    # Print metrics\n",
    "    print(f'Epoch {epoch}/{epochs}')\n",
    "    print(f'Training Loss: {avg_train_loss}')\n",
    "    print(f'Validation Loss: {val_loss}')\n",
    "    print(f'Validation F1 Score: {val_f1}')\n",
    "\n",
    "    output_dir = 'HasilModel'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model in .pt format\n",
    "    model_save_path = os.path.join(output_dir, f'finetuned_BERT_epoch_{epoch}.pt')\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "# Save final model in .pt format\n",
    "final_model_path = os.path.join(output_dir, 'model_final.pt')\n",
    "torch.save(model.state_dict(), final_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
